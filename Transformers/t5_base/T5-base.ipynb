{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def clear_cuda_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"CUDA cache cleared and memory freed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 01:25:46.548464: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-06 01:25:46.565218: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743902746.585703    7321 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743902746.591963    7321 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-06 01:25:46.613267: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import nltk\n",
    "from typing import List, Tuple\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from accelerate) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (0.4.3)\n",
      "Requirement already satisfied: nltk in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: rouge_score in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: bert_score in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (0.3.13)\n",
      "Requirement already satisfied: transformers[torch] in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (4.39.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from evaluate) (2.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from evaluate) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from evaluate) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from evaluate) (0.22.2)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: click in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: absl-py in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from bert_score) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from bert_score) (3.8.4)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from transformers[torch]) (3.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from transformers[torch]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from transformers[torch]) (1.6.0)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert_score) (12.5.82)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from matplotlib->bert_score) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from matplotlib->bert_score) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from matplotlib->bert_score) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from matplotlib->bert_score) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from matplotlib->bert_score) (3.1.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/anaconda3/envs/tf-gpu/lib/python3.11/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install evaluate nltk rouge_score bert_score transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data preparation and making decisions for additional tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁brick', ':', 'N', 'atura', 'l', '_', 'G', 'a', 's', '▁', 'r', 'd', 'f', 's', ':', 'l', 'abel', '▁', '?', 'l', 'abel', '▁', 'SEL', 'ECT', '▁W', 'HER', 'E', '▁brick', ':', 'Air', '_', 'Flow', '_', 'S', 'en', 's', 'or']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.tokenize(\n",
    "        \"brick:Natural_Gas rdfs:label ?label SELECT WHERE brick:Air_Flow_Sensor\"\n",
    "    )\n",
    ")\n",
    "# Output: ['brick', ':', 'Natural', '_', 'Gas', 'rdfs', ':', 'label', '?', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files merged successfully into 'merged_output.json' with keys: 'question', 'entity', and 'sparql'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the first JSON file\n",
    "with open(\"updated_bldg_question_pairs_entities.json\", \"r\") as f1:\n",
    "    data1 = json.load(f1)\n",
    "\n",
    "# Load the second JSON file\n",
    "with open(\"updated_combined_output_with_entity.json\", \"r\") as f2:\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "\n",
    "# Function to filter and extract only the required keys\n",
    "def filter_entry(entry):\n",
    "    return {\n",
    "        \"question\": entry[\"question\"],\n",
    "        \"entity\": entry[\"entity\"],\n",
    "        \"sparql\": entry[\"sparql\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Process both datasets and combine them\n",
    "merged_data = []\n",
    "\n",
    "# Filter entries from file1.json\n",
    "for entry in data1:\n",
    "    filtered_entry = filter_entry(entry)\n",
    "    merged_data.append(filtered_entry)\n",
    "\n",
    "# Filter entries from file2.json\n",
    "for entry in data2:\n",
    "    filtered_entry = filter_entry(entry)\n",
    "    merged_data.append(filtered_entry)\n",
    "\n",
    "# Save the merged dataset to a new JSON file\n",
    "with open(\"merged_output1.json\", \"w\") as f:\n",
    "    json.dump(merged_data, f, indent=4)\n",
    "\n",
    "print(\n",
    "    \"Files merged successfully into 'merged_output.json' with keys: 'question', 'entity', and 'sparql'.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files merged successfully into 'merged_output.json' with keys: 'question', 'entity', and 'sparql'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the first JSON file\n",
    "with open(\"abacws_bldg_question_pairs_entities.json\", \"r\") as f1:\n",
    "    data1 = json.load(f1)\n",
    "\n",
    "# Load the second JSON file\n",
    "with open(\"abacws_bldg_timeseries_question_pairs_entities.json\", \"r\") as f2:\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "#  # Load the second JSON file\n",
    "# with open(\"updated_bldg_question_pairs_entities.json\", \"r\") as f3:\n",
    "#     data3 = json.load(f3)\n",
    "\n",
    "\n",
    "# Function to filter and extract only the required keys\n",
    "def filter_entry(entry):\n",
    "    return {\n",
    "        \"question\": entry[\"question\"],\n",
    "        \"entity\": entry[\"entity\"],\n",
    "        \"sparql\": entry[\"sparql\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Process both datasets and combine them\n",
    "merged_data = []\n",
    "\n",
    "# Filter entries from file1.json\n",
    "for entry in data1:\n",
    "    filtered_entry = filter_entry(entry)\n",
    "    merged_data.append(filtered_entry)\n",
    "\n",
    "# Filter entries from file2.json\n",
    "for entry in data2:\n",
    "    filtered_entry = filter_entry(entry)\n",
    "    merged_data.append(filtered_entry)\n",
    "\n",
    "# Filter entries from file2.json\n",
    "# for entry in data3:\n",
    "#     filtered_entry = filter_entry(entry)\n",
    "#     merged_data.append(filtered_entry)\n",
    "\n",
    "# Save the merged dataset to a new JSON file\n",
    "with open(\"merged_output2.json\", \"w\") as f:\n",
    "    json.dump(merged_data, f, indent=4)\n",
    "\n",
    "print(\n",
    "    \"Files merged successfully into 'merged_output.json' with keys: 'question', 'entity', and 'sparql'.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files merged successfully into 'merged_output.json' with keys: 'question', 'entity', and 'sparql'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the first JSON file\n",
    "with open(\"merged_output2.json\", \"r\") as f1:\n",
    "    data1 = json.load(f1)\n",
    "  \n",
    "# Load the second JSON file\n",
    "with open(\"merged_output3.json\", \"r\") as f2:\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "\n",
    "# Function to filter and extract only the required keys\n",
    "def filter_entry(entry):\n",
    "    return {\n",
    "        \"question\": entry[\"question\"],\n",
    "        \"entity\": entry[\"entity\"],\n",
    "        \"sparql\": entry[\"sparql\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Process both datasets and combine them\n",
    "merged_data = []\n",
    "\n",
    "# Filter entries from file1.json\n",
    "for entry in data1:\n",
    "    filtered_entry = filter_entry(entry)\n",
    "    merged_data.append(filtered_entry)\n",
    "\n",
    "# Filter entries from file2.json\n",
    "for entry in data2:\n",
    "    filtered_entry = filter_entry(entry)\n",
    "    merged_data.append(filtered_entry)\n",
    "\n",
    "# Save the merged dataset to a new JSON file\n",
    "with open(\"training_data.json\", \"w\") as f:\n",
    "    json.dump(merged_data, f, indent=4)\n",
    "\n",
    "print(\n",
    "    \"Files merged successfully into 'merged_output.json' with keys: 'question', 'entity', and 'sparql'.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generated and saved to 'sparql_dataset1.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Function to read names from a file (one per line)\n",
    "def load_names_from_file(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        # Strip whitespace and filter out empty lines\n",
    "        names = [line.strip() for line in f if line.strip()]\n",
    "    return names\n",
    "\n",
    "\n",
    "# Load sensors and zones from files\n",
    "sensors = load_names_from_file(\"brick_sensors.txt\")\n",
    "zones = load_names_from_file(\"bldg_zones.txt\")\n",
    "\n",
    "# Add 'brick:' prefix to sensors and 'bldg:' prefix to zones if not already present\n",
    "sensors = [f\"brick:{s}\" if not s.startswith(\"brick:\") else s for s in sensors]\n",
    "zones = [f\"bldg:{z}\" if not z.startswith(\"bldg:\") else z for z in zones]\n",
    "\n",
    "\n",
    "# Function to generate SPARQL queries and dataset entries\n",
    "def generate_sparql_dataset(sensors_list, zones_list):\n",
    "    dataset = []\n",
    "\n",
    "    # Base entity template (e.g., \"bldg:<Zone> \\n brick:<SensorName>\")\n",
    "    def get_entity(sensor, zone):\n",
    "        sensor_name = sensor.split(\":\")[1]  # Extract sensor name after \"brick:\"\n",
    "        zone_name = zone.split(\":\")[1]  # Extract zone name after \"bldg:\"\n",
    "        return f\"bldg:{zone_name} \\n brick:{sensor_name}\"  # Newline-separated format\n",
    "\n",
    "    # Loop over each sensor and each zone\n",
    "    for sensor in sensors_list:\n",
    "        for zone in zones_list:\n",
    "            selected_entity = get_entity(sensor, zone)\n",
    "\n",
    "            # Create the question string with sensor and zone details\n",
    "            question = (\n",
    "                f\"Tell me the name or label of the {sensor.split(':')[1].replace('_', ' ').lower()} \"\n",
    "                f\"in the {zone.split(':')[1].replace('-', ' ').replace('_', ' ')}.\"\n",
    "            )\n",
    "\n",
    "            # Create the corresponding SPARQL query\n",
    "            sparql_query = f\"SELECT ?label WHERE {{ ?sensor a {sensor} ; brick:hasLocation {zone} ; rdfs:label ?label . }}\"\n",
    "\n",
    "            # Create an entry and append it to the dataset\n",
    "            entry = {\n",
    "                \"question\": question,\n",
    "                \"entity\": selected_entity,\n",
    "                \"sparql\": sparql_query,\n",
    "            }\n",
    "            dataset.append(entry)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Generate the dataset (will produce 5000 entries if there are 100 sensors and 50 zones)\n",
    "dataset = generate_sparql_dataset(sensors, zones)\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"merged_output3.json\", \"w\") as f:\n",
    "    json.dump(dataset, f, indent=4)\n",
    "\n",
    "print(\"Dataset generated and saved to 'sparql_dataset1.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 52599 records from updated_combined_output_with_entity.json\n",
      "Example record: {'question': 'What is the area of building bldg1?', 'entity': 'brick:area', 'sparql': 'SELECT ?value ?unit WHERE { bldg:bldg1 brick:area ?area . ?area brick:value ?value . ?area brick:hasUnits ?unit . }', 'sparql_response': '[{\"value\": \"9973^^xsd:integer\", \"unit\": \"FT_2\"}]', 'explanation': 'The area of building bldg1 is 9,973 square feet (FT_2). This means that the total floor space occupied by this building is approximately 9,973 square feet. This measurement is commonly used for real estate and construction purposes to describe the size of a building or space. In this case, the number \"9,973\" represents the precise area of building bldg1 as obtained from the smart building data created using Brickschema ontology. The \"FT_2\" notation signifies that the unit of measurement is square feet (foot squared).', 'id': 1}\n",
      "Generated 101897 total training pairs from 52599 records.\n",
      "Train size: 91707 | Validation size: 10190\n",
      "Saved train_data_2April.json and val_data_2April.json!\n",
      "Train sample: {'input_text': \"task: generate_sparql\\ninput: Define 'Emergency Phone'.\\nentitybrick:Emergency_Phone\", 'target_text': 'SELECT ?definition WHERE { brick:Emergency_Phone skos:definition ?definition . }'}\n",
      "Validation sample: {'input_text': 'task: generate_sparql\\ninput: Explain what Interface means.\\nentitybrick:Interface', 'target_text': 'SELECT ?definition WHERE { brick:Interface skos:definition ?definition . }'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3157 new tokens to the tokenizer!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9033cb32e2534517aefc4f2324c279f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/91707 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85146b3e756b42d8979b8f464b30c65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='146955' max='343920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [146955/343920 7:49:31 < 10:29:19, 5.22 it/s, Epoch 12.82/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Bertscore Precision</th>\n",
       "      <th>Bertscore Recall</th>\n",
       "      <th>Bertscore F1</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.323800</td>\n",
       "      <td>0.661418</td>\n",
       "      <td>0.244202</td>\n",
       "      <td>0.099510</td>\n",
       "      <td>0.233062</td>\n",
       "      <td>0.233117</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.192074</td>\n",
       "      <td>0.855569</td>\n",
       "      <td>0.807940</td>\n",
       "      <td>0.830578</td>\n",
       "      <td>18.784396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.587053</td>\n",
       "      <td>0.280404</td>\n",
       "      <td>0.125103</td>\n",
       "      <td>0.269910</td>\n",
       "      <td>0.270053</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.278528</td>\n",
       "      <td>0.882677</td>\n",
       "      <td>0.812443</td>\n",
       "      <td>0.845823</td>\n",
       "      <td>18.974092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.626500</td>\n",
       "      <td>0.544086</td>\n",
       "      <td>0.281515</td>\n",
       "      <td>0.134946</td>\n",
       "      <td>0.271180</td>\n",
       "      <td>0.271287</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.269733</td>\n",
       "      <td>0.885476</td>\n",
       "      <td>0.812579</td>\n",
       "      <td>0.847177</td>\n",
       "      <td>18.958292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.588100</td>\n",
       "      <td>0.512603</td>\n",
       "      <td>0.281397</td>\n",
       "      <td>0.135671</td>\n",
       "      <td>0.271331</td>\n",
       "      <td>0.271424</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>0.885017</td>\n",
       "      <td>0.812548</td>\n",
       "      <td>0.846957</td>\n",
       "      <td>18.962022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.558900</td>\n",
       "      <td>0.488756</td>\n",
       "      <td>0.313850</td>\n",
       "      <td>0.184582</td>\n",
       "      <td>0.304589</td>\n",
       "      <td>0.304690</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.302181</td>\n",
       "      <td>0.888935</td>\n",
       "      <td>0.817713</td>\n",
       "      <td>0.851505</td>\n",
       "      <td>18.970167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.536200</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>0.319792</td>\n",
       "      <td>0.195133</td>\n",
       "      <td>0.310231</td>\n",
       "      <td>0.310288</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.309091</td>\n",
       "      <td>0.887229</td>\n",
       "      <td>0.818980</td>\n",
       "      <td>0.851352</td>\n",
       "      <td>18.977625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.518000</td>\n",
       "      <td>0.454499</td>\n",
       "      <td>0.325723</td>\n",
       "      <td>0.198744</td>\n",
       "      <td>0.314771</td>\n",
       "      <td>0.314795</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.318325</td>\n",
       "      <td>0.885552</td>\n",
       "      <td>0.822105</td>\n",
       "      <td>0.852180</td>\n",
       "      <td>18.990186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.503300</td>\n",
       "      <td>0.442119</td>\n",
       "      <td>0.329566</td>\n",
       "      <td>0.209941</td>\n",
       "      <td>0.320139</td>\n",
       "      <td>0.320037</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.322040</td>\n",
       "      <td>0.884684</td>\n",
       "      <td>0.823457</td>\n",
       "      <td>0.852467</td>\n",
       "      <td>18.990285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.491100</td>\n",
       "      <td>0.431673</td>\n",
       "      <td>0.338236</td>\n",
       "      <td>0.223992</td>\n",
       "      <td>0.329603</td>\n",
       "      <td>0.329625</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.327153</td>\n",
       "      <td>0.885391</td>\n",
       "      <td>0.824366</td>\n",
       "      <td>0.853273</td>\n",
       "      <td>18.987929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.480900</td>\n",
       "      <td>0.423087</td>\n",
       "      <td>0.347757</td>\n",
       "      <td>0.234342</td>\n",
       "      <td>0.339611</td>\n",
       "      <td>0.339731</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.337299</td>\n",
       "      <td>0.886209</td>\n",
       "      <td>0.825573</td>\n",
       "      <td>0.854318</td>\n",
       "      <td>18.986457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.415593</td>\n",
       "      <td>0.357006</td>\n",
       "      <td>0.243069</td>\n",
       "      <td>0.349082</td>\n",
       "      <td>0.349172</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.346878</td>\n",
       "      <td>0.888039</td>\n",
       "      <td>0.827019</td>\n",
       "      <td>0.855952</td>\n",
       "      <td>18.990579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.464600</td>\n",
       "      <td>0.409224</td>\n",
       "      <td>0.364635</td>\n",
       "      <td>0.251659</td>\n",
       "      <td>0.357101</td>\n",
       "      <td>0.357267</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.352970</td>\n",
       "      <td>0.889986</td>\n",
       "      <td>0.827788</td>\n",
       "      <td>0.857274</td>\n",
       "      <td>18.987341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def clear_cuda_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"CUDA cache cleared and memory freed.\")\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback\n",
    "import gc\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq\n",
    "\n",
    "# Set CUDA device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# 1. LOAD AND PREPARE DATA\n",
    "DATA_FILE = \"updated_combined_output_with_entity.json\"\n",
    "with open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "print(f\"Loaded {len(data)} records from {DATA_FILE}\")\n",
    "print(\"Example record:\", data[0])\n",
    "\n",
    "# Create multi-task training pairs\n",
    "inputs, targets = [], []\n",
    "for record in data:\n",
    "    question = record.get(\"question\", \"\")\n",
    "    entity = record.get(\"entity\", \"\")\n",
    "    sparql = record.get(\"sparql\", \"\")\n",
    "    # response = record.get(\"sparql_response\", \"\")\n",
    "    # explanation = record.get(\"explanation\", \"\")\n",
    "\n",
    "    # Task 1: NL to SPARQL\n",
    "    if question and entity and sparql:\n",
    "        inputs.append(f\"task: generate_sparql\\ninput: {question}\\nentity{entity}\")\n",
    "        targets.append(sparql)\n",
    "\n",
    "    # # Task 2: Summarize response\n",
    "    # if question and response and explanation:\n",
    "    #     inputs.append(f\"task: summarize_response\\nquestion: {question}\\nresponse: {response}\")\n",
    "    #     targets.append(explanation)\n",
    "\n",
    "print(f\"Generated {len(inputs)} total training pairs from {len(data)} records.\")\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(\n",
    "    inputs, targets, test_size=0.1, random_state=42\n",
    ")\n",
    "print(f\"Train size: {len(train_inputs)} | Validation size: {len(val_inputs)}\")\n",
    "\n",
    "# Save splits\n",
    "train_data = [\n",
    "    {\"input_text\": inp, \"target_text\": tgt}\n",
    "    for inp, tgt in zip(train_inputs, train_targets)\n",
    "]\n",
    "val_data = [\n",
    "    {\"input_text\": inp, \"target_text\": tgt} for inp, tgt in zip(val_inputs, val_targets)\n",
    "]\n",
    "with open(\"train_data_2April.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "with open(\"val_data_2April.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved train_data_2April.json and val_data_2April.json!\")\n",
    "\n",
    "# Build datasets\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_dict(\n",
    "            {\"input_text\": train_inputs, \"target_text\": train_targets}\n",
    "        ),\n",
    "        \"validation\": Dataset.from_dict(\n",
    "            {\"input_text\": val_inputs, \"target_text\": val_targets}\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "print(\"Train sample:\", raw_datasets[\"train\"][3])\n",
    "print(\"Validation sample:\", raw_datasets[\"validation\"][3])\n",
    "\n",
    "# 2. LOAD MODEL & TOKENIZER\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Add custom tokens once\n",
    "# custom_tokens = []\n",
    "# with open(\"all_relations_and_classes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     custom_tokens.extend([line.strip() for line in f.readlines()])\n",
    "# with open(\"output_entities.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     custom_tokens.extend([line.strip() for line in f.readlines()])\n",
    "# num_added_tokens = tokenizer.add_tokens(custom_tokens)\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# print(f\"Added {num_added_tokens} new tokens to the tokenizer!\")\n",
    "\n",
    "\n",
    "# 3. PREPROCESSING\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input_text\"], max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"target_text\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# 4. EVALUATION METRICS\n",
    "metric_rouge = evaluate.load(\"rouge\")\n",
    "metric_bleu = evaluate.load(\"bleu\")\n",
    "metric_meteor = evaluate.load(\"meteor\")\n",
    "metric_bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    results = {}\n",
    "    rouge_result = metric_rouge.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    results.update(rouge_result)\n",
    "    bleu_result = metric_bleu.compute(\n",
    "        predictions=decoded_preds, references=[[label] for label in decoded_labels]\n",
    "    )\n",
    "    results[\"bleu\"] = bleu_result[\"bleu\"]\n",
    "    meteor_result = metric_meteor.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels\n",
    "    )\n",
    "    results[\"meteor\"] = meteor_result[\"meteor\"]\n",
    "    bertscore_result = metric_bertscore.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, lang=\"en\"\n",
    "    )\n",
    "    results[\"bertscore_precision\"] = np.mean(bertscore_result[\"precision\"])\n",
    "    results[\"bertscore_recall\"] = np.mean(bertscore_result[\"recall\"])\n",
    "    results[\"bertscore_f1\"] = np.mean(bertscore_result[\"f1\"])\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    results[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return results\n",
    "\n",
    "\n",
    "# 5. TRAINING ARGUMENTS\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./training_t5small\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_dir=\"./training_t5small\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # Adjust based on GPU\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=30,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,  # Enable if GPU supports\n",
    "    report_to=[\"tensorboard\"],\n",
    "    warmup_steps=500,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "# 6. TRAINER SETUP\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# 7. TRAIN\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 52599 records from updated_combined_output_with_entity.json\n",
      "Example record: {'question': 'What is the area of building bldg1?', 'entity': 'brick:area', 'sparql': 'SELECT ?value ?unit WHERE { bldg:bldg1 brick:area ?area . ?area brick:value ?value . ?area brick:hasUnits ?unit . }', 'sparql_response': '[{\"value\": \"9973^^xsd:integer\", \"unit\": \"FT_2\"}]', 'explanation': 'The area of building bldg1 is 9,973 square feet (FT_2). This means that the total floor space occupied by this building is approximately 9,973 square feet. This measurement is commonly used for real estate and construction purposes to describe the size of a building or space. In this case, the number \"9,973\" represents the precise area of building bldg1 as obtained from the smart building data created using Brickschema ontology. The \"FT_2\" notation signifies that the unit of measurement is square feet (foot squared).', 'id': 1}\n",
      "Generated 101897 total training pairs from 52599 records.\n",
      "Train size: 91707 | Validation size: 10190\n",
      "Saved train_data_2April.json and val_data_2April.json!\n",
      "Train sample: {'input_text': \"task: generate_sparql\\ninput: Does Radiant Ceiling Panel include the tag 'Ceiling'?\\nentitybrick:Radiant_Ceiling_Panel\", 'target_text': 'ASK WHERE { brick:Radiant_Ceiling_Panel brick:hasAssociatedTag tag:Ceiling . }'}\n",
      "Validation sample: {'input_text': 'task: summarize_response\\nquestion: How is the Enthalpy Setpoint categorized in the ontology?\\nresponse: [{\"superclass\": \"Setpoint\"}]', 'target_text': \"The Enthalpy Setpoint, as categorized in the ontology created using BrickSchema, belongs to a broader class of terms called Setpoints. In building automation and control systems, a setpoint is a specific value or condition that an automated system aims to maintain for the optimal functioning of a particular component or system.\\n\\nIn this case, the Enthalpy Setpoint refers to the targeted enthalpy level in a controlled environment such as a building. Entalpy is a thermodynamic property that describes the total energy (heat and internal energy) of a substance per unit mass. By setting an enthalpy setpoint, the system can control the heating or cooling process more accurately and efficiently to maintain the desired indoor comfort conditions.\\n\\nOverall, understanding the Enthalpy Setpoint's classification as a Setpoint in this ontology helps build a comprehensive model for building automation systems by providing clarity on its role, significance, and interactions with other components within the smart building ecosystem.\"}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e0dc3bd3a5433d8e0798642432308f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/91707 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4125eb0c6674b368f5396359a27f93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36562' max='343920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 36562/343920 1:53:29 < 15:54:07, 5.37 it/s, Epoch 3.19/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Bertscore Precision</th>\n",
       "      <th>Bertscore Recall</th>\n",
       "      <th>Bertscore F1</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.248385</td>\n",
       "      <td>0.504155</td>\n",
       "      <td>0.433181</td>\n",
       "      <td>0.496060</td>\n",
       "      <td>0.496050</td>\n",
       "      <td>0.005084</td>\n",
       "      <td>0.480016</td>\n",
       "      <td>0.934897</td>\n",
       "      <td>0.888933</td>\n",
       "      <td>0.910661</td>\n",
       "      <td>18.019725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.294300</td>\n",
       "      <td>0.248268</td>\n",
       "      <td>0.507756</td>\n",
       "      <td>0.438752</td>\n",
       "      <td>0.499885</td>\n",
       "      <td>0.499957</td>\n",
       "      <td>0.005157</td>\n",
       "      <td>0.482675</td>\n",
       "      <td>0.935279</td>\n",
       "      <td>0.889683</td>\n",
       "      <td>0.911232</td>\n",
       "      <td>18.021295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>0.248041</td>\n",
       "      <td>0.511299</td>\n",
       "      <td>0.443744</td>\n",
       "      <td>0.503168</td>\n",
       "      <td>0.503253</td>\n",
       "      <td>0.005315</td>\n",
       "      <td>0.484156</td>\n",
       "      <td>0.936100</td>\n",
       "      <td>0.890650</td>\n",
       "      <td>0.912128</td>\n",
       "      <td>18.020510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 181\u001b[0m\n\u001b[1;32m    169\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m    170\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    171\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)]\n\u001b[1;32m    178\u001b[0m )\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# 7. TRAIN\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/accelerate/accelerator.py:2242\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2242\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def clear_cuda_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"CUDA cache cleared and memory freed.\")\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, EarlyStoppingCallback\n",
    "import gc\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq\n",
    "\n",
    "# Set CUDA device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# 1. LOAD AND PREPARE DATA\n",
    "DATA_FILE = \"updated_combined_output_with_entity.json\"\n",
    "with open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "print(f\"Loaded {len(data)} records from {DATA_FILE}\")\n",
    "print(\"Example record:\", data[0])\n",
    "\n",
    "# Create multi-task training pairs\n",
    "inputs, targets = [], []\n",
    "for record in data:\n",
    "    question = record.get(\"question\", \"\")\n",
    "    entity = record.get(\"entity\", \"\")\n",
    "    sparql = record.get(\"sparql\", \"\")\n",
    "    response = record.get(\"sparql_response\", \"\")\n",
    "    explanation = record.get(\"explanation\", \"\")\n",
    "\n",
    "    # Task 1: NL to SPARQL\n",
    "    if question and entity and sparql:\n",
    "        inputs.append(f\"task: generate_sparql\\ninput: {question}\\nentity{entity}\")\n",
    "        targets.append(sparql)\n",
    "\n",
    "    # Task 2: Summarize response\n",
    "    if question and response and explanation:\n",
    "        inputs.append(\n",
    "            f\"task: summarize_response\\nquestion: {question}\\nresponse: {response}\"\n",
    "        )\n",
    "        targets.append(explanation)\n",
    "\n",
    "print(f\"Generated {len(inputs)} total training pairs from {len(data)} records.\")\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(\n",
    "    inputs, targets, test_size=0.1, random_state=40\n",
    ")\n",
    "print(f\"Train size: {len(train_inputs)} | Validation size: {len(val_inputs)}\")\n",
    "\n",
    "# Save splits\n",
    "train_data = [\n",
    "    {\"input_text\": inp, \"target_text\": tgt}\n",
    "    for inp, tgt in zip(train_inputs, train_targets)\n",
    "]\n",
    "val_data = [\n",
    "    {\"input_text\": inp, \"target_text\": tgt} for inp, tgt in zip(val_inputs, val_targets)\n",
    "]\n",
    "with open(\"train_data_2April.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "with open(\"val_data_2April.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved train_data_2April.json and val_data_2April.json!\")\n",
    "\n",
    "# Build datasets\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_dict(\n",
    "            {\"input_text\": train_inputs, \"target_text\": train_targets}\n",
    "        ),\n",
    "        \"validation\": Dataset.from_dict(\n",
    "            {\"input_text\": val_inputs, \"target_text\": val_targets}\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "print(\"Train sample:\", raw_datasets[\"train\"][3])\n",
    "print(\"Validation sample:\", raw_datasets[\"validation\"][3])\n",
    "\n",
    "# 2. LOAD MODEL & TOKENIZER\n",
    "model_name = \"./training_t5smallv2/checkpoint-343920\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Add custom tokens once\n",
    "# custom_tokens = []\n",
    "# with open(\"all_relations_and_classes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     custom_tokens.extend([line.strip() for line in f.readlines()])\n",
    "# with open(\"output_entities.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     custom_tokens.extend([line.strip() for line in f.readlines()])\n",
    "# num_added_tokens = tokenizer.add_tokens(custom_tokens)\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# print(f\"Added {num_added_tokens} new tokens to the tokenizer!\")\n",
    "\n",
    "\n",
    "# 3. PREPROCESSING\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input_text\"], max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"target_text\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# 4. EVALUATION METRICS\n",
    "metric_rouge = evaluate.load(\"rouge\")\n",
    "metric_bleu = evaluate.load(\"bleu\")\n",
    "metric_meteor = evaluate.load(\"meteor\")\n",
    "metric_bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    results = {}\n",
    "    rouge_result = metric_rouge.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    results.update(rouge_result)\n",
    "    bleu_result = metric_bleu.compute(\n",
    "        predictions=decoded_preds, references=[[label] for label in decoded_labels]\n",
    "    )\n",
    "    results[\"bleu\"] = bleu_result[\"bleu\"]\n",
    "    meteor_result = metric_meteor.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels\n",
    "    )\n",
    "    results[\"meteor\"] = meteor_result[\"meteor\"]\n",
    "    bertscore_result = metric_bertscore.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, lang=\"en\"\n",
    "    )\n",
    "    results[\"bertscore_precision\"] = np.mean(bertscore_result[\"precision\"])\n",
    "    results[\"bertscore_recall\"] = np.mean(bertscore_result[\"recall\"])\n",
    "    results[\"bertscore_f1\"] = np.mean(bertscore_result[\"f1\"])\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    results[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return results\n",
    "\n",
    "\n",
    "# 5. TRAINING ARGUMENTS\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./training_t5small\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_dir=\"./training_t5small\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # Adjust based on GPU\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=30,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,  # Enable if GPU supports\n",
    "    report_to=[\"tensorboard\"],\n",
    "    warmup_steps=500,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "# 6. TRAINER SETUP\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# 7. TRAIN\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.8/dist-packages (0.4.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.9.1)\n",
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.8/dist-packages (0.1.2)\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 7.0 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.24.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.20.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.23.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2024.6.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from rouge_score) (1.3.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from bert_score) (3.6.2)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from bert_score) (2.4.0)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from bert_score) (4.43.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (3.10.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->bert_score) (1.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->bert_score) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->bert_score) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->bert_score) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->bert_score) (9.3.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->bert_score) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->bert_score) (12.1.0.106)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->bert_score) (3.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->bert_score) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->bert_score) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->bert_score) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->bert_score) (11.4.5.107)\n",
      "Requirement already satisfied: triton==3.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.13\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->bert_score) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->bert_score) (2.20.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.8/dist-packages (from transformers>=3.0.0->bert_score) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=3.0.0->bert_score) (0.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.3.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0; python_version < \"3.11\" in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.8/dist-packages (from nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.0.0->bert_score) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Installing collected packages: bert-score\n",
      "Successfully installed bert-score-0.3.13\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n",
      "Loaded 10190 validation records from val_data_2April.json\n",
      "SPARQL task: 5187 examples\n",
      "Summarization task: 5003 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating SPARQL Generation Task ===\n",
      "Example 1:\n",
      "Input:      task: generate_sparql\n",
      "input: What types is the Occupied Heating Mode Status?\n",
      "entitybrick:Occupied_Heating_Mode_Status\n",
      "Prediction: SELECT?definition WHERE?quantity skos:definition?definition.\n",
      "Target:     SELECT ?type WHERE { brick:Occupied_Heating_Mode_Status a ?type . }\n",
      "--------------------------------------------------\n",
      "Example 2:\n",
      "Input:      task: generate_sparql\n",
      "input: Provide the definition for Air Flow Setpoint.\n",
      "entitybrick:Air_Flow_Setpoint\n",
      "Prediction: SELECT?definition WHERE brick:Fan skos:definition?definition.\n",
      "Target:     SELECT ?definition WHERE { brick:Air_Flow_Setpoint skos:definition ?definition . }\n",
      "--------------------------------------------------\n",
      "Example 3:\n",
      "Input:      task: generate_sparql\n",
      "input: Can you provide the definition for the Return Air Humidity Sensor?\n",
      "entitybrick:Return_Air_Humidity_Sensor\n",
      "Prediction: SELECT?definition WHERE?parent skos:definition?definition.\n",
      "Target:     SELECT ?definition WHERE { brick:Return_Air_Humidity_Sensor skos:definition ?definition . }\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f364a76bae4bb6acf7a1abecd9023d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170d5df2438a4da8911004c94562b657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab189eb256a3422584e2e27e541cef07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07874690878040f6a84b4a997091386e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c990b85faca4133bf2b69d4655b416c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ad0627504c4d778b74cf3a5da5354a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPARQL Generation Metrics:\n",
      "sparql_rouge_rouge1: 0.6714\n",
      "sparql_rouge_rouge2: 0.5177\n",
      "sparql_rouge_rougeL: 0.6709\n",
      "sparql_rouge_rougeLsum: 0.6709\n",
      "sparql_bleu: 0.3783\n",
      "sparql_meteor: 0.6631\n",
      "sparql_bertscore_precision: 0.9139\n",
      "sparql_bertscore_recall: 0.8740\n",
      "sparql_bertscore_f1: 0.8934\n",
      "sparql_gen_len: 3.8359\n",
      "\n",
      "=== Evaluating Summarization Task ===\n",
      "Example 1:\n",
      "Input:      task: summarize_response\n",
      "question: What is the quantity linked to the Return Air Enthalpy Sensor?\n",
      "response: [{\"quantity\": \"Enthalpy\"}]\n",
      "Prediction: The Return Air Enthalpy Sensor in this smart building system measures the enthalpy of the air being returned to the HVAC (Heating, Ventilation, and Air Conditioning) system. Enthalpy is a thermodynamic property that describes the total energy content of a substance (in this case, air) in a given volume of air. In simpler terms, the Return Air Enthalpy Sensor measures the enthalpy of the air being returned to the heating, ventilation, and air conditioning (HVAC) system after it has circulated through the space being conditioned. This information is crucial for maintaining optimal indoor air quality and energy efficiency within a smart building.\n",
      "Target:     The quantity linked to the Return Air Enthalpy Sensor is Enthalpy. In a building's HVAC (Heating, Ventilation, and Air Conditioning) system, enthalpy refers to the sum of the internal energy plus the product of pressure and specific volume for a given substance. This value can help determine the amount of heat energy that is contained within a certain mass of air or fluid, which is essential for maintaining comfortable temperatures in a building.\n",
      "\n",
      "In simpler terms, the Return Air Enthalpy Sensor measures the total heat content (combining internal energy and pressure) of the air being returned to the HVAC system. This information helps the HVAC system adjust itself accordingly, ensuring that appropriate cooling or heating is supplied as needed.\n",
      "--------------------------------------------------\n",
      "Example 2:\n",
      "Input:      task: summarize_response\n",
      "question: What is the name of Unoccupied Supply Air Flow Setpoint?\n",
      "response: [{\"label\": \"Unoccupied Supply Air Flow Setpoint\"}]\n",
      "Prediction: The term \"Unoccupied Supply Air Flow Setpoint\" refers to a specific setting in a smart building system. This setpoint is used to control the amount of air that should be supplied into an unoccupied space, such as a room or office. In simpler terms, it's the target amount of air that should be supplied into a space when no one is present in the building. This value is crucial for energy efficiency and comfort when the building is unoccupied.\n",
      "Target:     The Unoccupied Supply Air Flow Setpoint refers to a specific setting or target value for the amount of air that is supplied into a building when it's unoccupied. This value helps control the indoor environment and maintain comfortable conditions while saving energy by minimizing unnecessary heating, cooling, or ventilation. In other words, it's a predefined parameter for maintaining an optimal balance between energy efficiency and comfort levels when no one is using the space.\n",
      "--------------------------------------------------\n",
      "Example 3:\n",
      "Input:      task: summarize_response\n",
      "question: What is the equivalent class of Min Unoccupied Heating Supply Air Flow Setpoint Limit?\n",
      "response: [{\"equiv\": \"Min_Unoccupied_Heating_Discharge_Air_Flow_Setpoint_Limit\"}]\n",
      "Prediction: The Min Unoccupied Heating Supply Air Flow Setpoint Limit belongs to the class called \"Min_Unoccupied_Heating_Discharge_Air_Flow_Setpoint_Limit\". In simpler terms, this means that the Min Unoccupied Heating Supply Air Flow Setpoint Limit is a specific type or instance of the same class in the smart building ontology created using BrickSchema. The term \"Min_Unoccupied_Heating_Discharge_Air_Flow_Setpoint_Limit\" refers to the minimum amount of heated air that should be supplied when a space is unoccupied. This limit ensures that the heating system doesn't overheat when the space is unoccupied.\n",
      "Target:     The Min Unoccupied Heating Supply Air Flow Setpoint Limit belongs to the class named \"Min_Unoccupied_Heating_Discharge_Air_Flow_Setpoint_Limit\". This class represents a specific type of limit on the minimum amount of heated air flow that should be supplied when a building is unoccupied. It's an essential part of a smart building system, helping to optimize energy usage and maintain comfortable temperatures during periods of absence. The setpoint limit ensures that the heating system doesn't use more energy than necessary while the building is empty or minimally occupied.\n",
      "--------------------------------------------------\n",
      "\n",
      "Summarization Metrics:\n",
      "summarization_rouge_rouge1: 0.5542\n",
      "summarization_rouge_rouge2: 0.2751\n",
      "summarization_rouge_rougeL: 0.3622\n",
      "summarization_rouge_rougeLsum: 0.4157\n",
      "summarization_bleu: 0.1869\n",
      "summarization_meteor: 0.3544\n",
      "summarization_bertscore_precision: 0.9033\n",
      "summarization_bertscore_recall: 0.8834\n",
      "summarization_bertscore_f1: 0.8932\n",
      "summarization_gen_len: 108.3038\n",
      "CUDA cache cleared.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5ForConditionalGeneration\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback,\n",
    "    T5Tokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import gc\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. LOAD THE FINE-TUNED MODEL AND TOKENIZER\n",
    "# -----------------------------------------------------------------------------\n",
    "model_name = \"./training_t5small/checkpoint-343920\"  # Use your latest checkpoint\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. LOAD EVALUATION DATA\n",
    "# -----------------------------------------------------------------------------\n",
    "# Option 1: Load from validation split saved during training\n",
    "val_data_file = \"val_data_2April.json\"\n",
    "with open(val_data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    val_data = json.load(f)\n",
    "print(f\"Loaded {len(val_data)} validation records from {val_data_file}\")\n",
    "\n",
    "# Separate tasks\n",
    "sparql_inputs = []\n",
    "sparql_targets = []\n",
    "summarization_inputs = []\n",
    "summarization_targets = []\n",
    "\n",
    "for record in val_data:\n",
    "    input_text = record[\"input_text\"]\n",
    "    target_text = record[\"target_text\"]\n",
    "\n",
    "    if \"task: generate_sparql\" in input_text:\n",
    "        sparql_inputs.append(input_text)\n",
    "        sparql_targets.append(target_text)\n",
    "    elif \"task: summarize_response\" in input_text:\n",
    "        summarization_inputs.append(input_text)\n",
    "        summarization_targets.append(target_text)\n",
    "\n",
    "print(f\"SPARQL task: {len(sparql_inputs)} examples\")\n",
    "print(f\"Summarization task: {len(summarization_inputs)} examples\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. DEFINE HELPER FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "def generate_predictions(input_texts, max_length=512):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for text in input_texts:\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "            ).to(device)\n",
    "            output_ids = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=max_length,\n",
    "                num_beams=4,  # Beam search for better quality\n",
    "                early_stopping=True,\n",
    "            )\n",
    "            pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            predictions.append(pred.strip())\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. LOAD EVALUATION METRICS\n",
    "# -----------------------------------------------------------------------------\n",
    "metric_rouge = evaluate.load(\"rouge\")\n",
    "metric_bleu = evaluate.load(\"bleu\")\n",
    "metric_meteor = evaluate.load(\"meteor\")\n",
    "metric_bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, references, task_name=\"\"):\n",
    "    results = {}\n",
    "    # ROUGE\n",
    "    rouge_result = metric_rouge.compute(\n",
    "        predictions=predictions, references=references, use_stemmer=True\n",
    "    )\n",
    "    results.update({f\"{task_name}_rouge_{k}\": v for k, v in rouge_result.items()})\n",
    "\n",
    "    # BLEU\n",
    "    bleu_result = metric_bleu.compute(\n",
    "        predictions=predictions, references=[[ref] for ref in references]\n",
    "    )\n",
    "    results[f\"{task_name}_bleu\"] = bleu_result[\"bleu\"]\n",
    "\n",
    "    # METEOR\n",
    "    meteor_result = metric_meteor.compute(\n",
    "        predictions=predictions, references=references\n",
    "    )\n",
    "    results[f\"{task_name}_meteor\"] = meteor_result[\"meteor\"]\n",
    "\n",
    "    # BERTScore\n",
    "    bertscore_result = metric_bertscore.compute(\n",
    "        predictions=predictions, references=references, lang=\"en\"\n",
    "    )\n",
    "    results[f\"{task_name}_bertscore_precision\"] = np.mean(bertscore_result[\"precision\"])\n",
    "    results[f\"{task_name}_bertscore_recall\"] = np.mean(bertscore_result[\"recall\"])\n",
    "    results[f\"{task_name}_bertscore_f1\"] = np.mean(bertscore_result[\"f1\"])\n",
    "\n",
    "    # Average generation length\n",
    "    prediction_lens = [len(pred.split()) for pred in predictions]\n",
    "    results[f\"{task_name}_gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. EVALUATE SPARQL GENERATION TASK\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n=== Evaluating SPARQL Generation Task ===\")\n",
    "sparql_predictions = generate_predictions(sparql_inputs, max_length=128)\n",
    "\n",
    "# Print a few examples\n",
    "for i, (inp, pred, tgt) in enumerate(\n",
    "    zip(sparql_inputs[:3], sparql_predictions[:3], sparql_targets[:3])\n",
    "):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(\"Input:     \", inp)\n",
    "    print(\"Prediction:\", pred)\n",
    "    print(\"Target:    \", tgt)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Compute metrics\n",
    "sparql_metrics = compute_metrics(sparql_predictions, sparql_targets, task_name=\"sparql\")\n",
    "print(\"\\nSPARQL Generation Metrics:\")\n",
    "for k, v in sparql_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. EVALUATE SUMMARIZATION TASK\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n=== Evaluating Summarization Task ===\")\n",
    "summarization_predictions = generate_predictions(summarization_inputs, max_length=256)\n",
    "\n",
    "# Print a few examples\n",
    "for i, (inp, pred, tgt) in enumerate(\n",
    "    zip(\n",
    "        summarization_inputs[:3],\n",
    "        summarization_predictions[:3],\n",
    "        summarization_targets[:3],\n",
    "    )\n",
    "):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(\"Input:     \", inp)\n",
    "    print(\"Prediction:\", pred)\n",
    "    print(\"Target:    \", tgt)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Compute metrics\n",
    "summarization_metrics = compute_metrics(\n",
    "    summarization_predictions, summarization_targets, task_name=\"summarization\"\n",
    ")\n",
    "print(\"\\nSummarization Metrics:\")\n",
    "for k, v in summarization_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. CLEAR CUDA CACHE (Optional)\n",
    "# -----------------------------------------------------------------------------\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test on new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. LOAD THE TRAINED MODEL AND TOKENIZER\n",
    "# -----------------------------------------------------------------------------\n",
    "model_name = \"./training_t5small/checkpoint-57320\"  # Update with your checkpoint path\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. HELPER FUNCTION TO GENERATE RESPONSES\n",
    "# -----------------------------------------------------------------------------\n",
    "def generate_response(input_text, max_length=256):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "        ).to(device)\n",
    "        output_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=max_length,\n",
    "            num_beams=4,  # Beam search for better quality\n",
    "            early_stopping=True,\n",
    "        )\n",
    "        response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. INTERACTIVE TESTING LOOP\n",
    "# -----------------------------------------------------------------------------\n",
    "def test_model():\n",
    "    print(\"\\n=== Interactive Model Testing ===\")\n",
    "    print(\"Enter inputs for SPARQL generation or summarization tasks.\")\n",
    "    print(\n",
    "        \"For SPARQL: Provide a natural language question (e.g., 'What is the capital of France?')\"\n",
    "    )\n",
    "    print(\n",
    "        \"For Summarization: Provide a question and response (e.g., 'What is the weather like? [Weather data]')\"\n",
    "    )\n",
    "    print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        # Get user input\n",
    "        task_choice = input(\"Choose task (1 for SPARQL, 2 for Summarization): \").strip()\n",
    "\n",
    "        if task_choice.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        if task_choice not in [\"1\", \"2\"]:\n",
    "            print(\"Invalid choice. Please enter 1 for SPARQL or 2 for Summarization.\")\n",
    "            continue\n",
    "\n",
    "        # Task 1: SPARQL Generation\n",
    "        if task_choice == \"1\":\n",
    "            question = input(\"Enter your question: \").strip()\n",
    "            if not question:\n",
    "                print(\"Please provide a question.\")\n",
    "                continue\n",
    "\n",
    "            # Add entity if provided (optional)\n",
    "            entity = input(\"Enter entity (optional, press Enter to skip): \").strip()\n",
    "            input_text = f\"task: generate_sparql\\ninput: {question}\"\n",
    "            if entity:\n",
    "                input_text += f\"\\nentity: {entity}\"\n",
    "\n",
    "            # Generate SPARQL\n",
    "            sparql_response = generate_response(input_text, max_length=128)\n",
    "            print(\"\\nGenerated SPARQL:\")\n",
    "            print(sparql_response)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        # Task 2: Summarization\n",
    "        elif task_choice == \"2\":\n",
    "            question = input(\"Enter your question: \").strip()\n",
    "            response = input(\"Enter the response to summarize: \").strip()\n",
    "            if not question or not response:\n",
    "                print(\"Please provide both a question and a response.\")\n",
    "                continue\n",
    "\n",
    "            input_text = (\n",
    "                f\"task: summarize_response\\nquestion: {question}\\nresponse: {response}\"\n",
    "            )\n",
    "\n",
    "            # Generate summary\n",
    "            summary_response = generate_response(input_text, max_length=256)\n",
    "            print(\"\\nGenerated Summary:\")\n",
    "            print(summary_response)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. RUN THE TEST\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    test_model()\n",
    "\n",
    "    # Clear CUDA cache after testing (optional)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"CUDA cache cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SPARQL Query:\n",
      "SELECT?definition WHERE brick:Heating_Ventilation_Air_Conditioning_System skos:definition?definition.\n",
      "\n",
      "Generated Summary:\n",
      "A Heating Ventilation Air Conditioning (HVAC) System refers to the collection of equipment, distribution systems, and terminals that provide, collectively or individually, the processes of heating, ventilating, or air conditioning to a specific building or portion of a building. This system is responsible for maintaining a comfortable indoor environment by regulating temperature, humidity, and air quality. The primary function of a HVAC system is to maintain a comfortable temperature for occupants while also ensuring energy efficiency.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load your trained model and tokenizer from the saved directory.\n",
    "model_dir = \"./training_t5smallv2/checkpoint-343920\"  # Update this path if needed\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def generate_sparql(question, entity):\n",
    "    \"\"\"\n",
    "    Generate a SPARQL query given a natural language question and an entity.\n",
    "    \"\"\"\n",
    "    # Format the input as it was during training.\n",
    "    input_text = f\"task: generate_sparql\\ninput: {question}\\nentity{entity}\"\n",
    "    input_ids = tokenizer.encode(\n",
    "        input_text, return_tensors=\"pt\", truncation=True, max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate output using beam search.\n",
    "    outputs = model.generate(\n",
    "        input_ids, max_length=150, num_beams=5, early_stopping=True\n",
    "    )\n",
    "    generated_sparql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_sparql\n",
    "\n",
    "\n",
    "def summarize_response(question, response):\n",
    "    \"\"\"\n",
    "    Generate a summary (explanation) based on the question and response text.\n",
    "    \"\"\"\n",
    "    # Format the input as it was during training.\n",
    "    input_text = f\"task: summarize_response\\nquestion: {question}\\nresponse: {response}\"\n",
    "    input_ids = tokenizer.encode(\n",
    "        input_text, return_tensors=\"pt\", truncation=True, max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate summary output.\n",
    "    outputs = model.generate(\n",
    "        input_ids, max_length=256, num_beams=5, early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Example input data\n",
    "input_data = {\n",
    "    \"question\": \"Provide the definition for Heating Ventilation Air Conditioning System.\",\n",
    "    \"entity\": \"brick:Heating_Ventilation_Air_Conditioning_System\",\n",
    "    \"sparql\": \"SELECT ?definition WHERE { brick:Heating_Ventilation_Air_Conditioning_System skos:definition ?definition . }\",\n",
    "    \"sparql_response\": '[{\"definition\": \"The equipment, distribution systems and terminals that provide, either collectively or individually, the processes of heating, ventilating or air conditioning to a building or portion of a building\"}]',\n",
    "    \"explanation\": (\n",
    "        \"The Heating Ventilation Air Conditioning (HVAC) System is a set of equipment, systems, and components that work together \"\n",
    "        \"to control and maintain comfortable conditions within a building. This includes processes such as heating the interior space during \"\n",
    "        \"cold weather, cooling it during warm weather, moving air for ventilation, and maintaining the desired indoor air quality. The HVAC \"\n",
    "        \"system is crucial for providing a comfortable and healthy environment in any building, be it residential or commercial. It can \"\n",
    "        \"consist of various components like air handlers, ducts, vents, thermostats, boilers, furnaces, cooling towers, heat pumps, and more, \"\n",
    "        \"depending on the specific needs of the building. The HVAC system is designed to ensure that the temperature, humidity, and air quality \"\n",
    "        \"within the building are maintained at optimal levels for the comfort and health of its occupants.\"\n",
    "    ),\n",
    "    \"id\": 49568,\n",
    "}\n",
    "\n",
    "# Get outputs for both tasks\n",
    "sparql_output = generate_sparql(input_data[\"question\"], input_data[\"entity\"])\n",
    "summary_output = summarize_response(\n",
    "    input_data[\"question\"], input_data[\"sparql_response\"]\n",
    ")\n",
    "\n",
    "print(\"Generated SPARQL Query:\")\n",
    "print(sparql_output)\n",
    "print(\"\\nGenerated Summary:\")\n",
    "print(summary_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bldg_sensors_dash.txt\n"
     ]
    }
   ],
   "source": [
    "def add_prefix(input_file: str, output_file: str):\n",
    "    with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            # Remove any trailing newline or whitespace\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                # Prepend \"bldg:\" to the line and write it to the output file\n",
    "                outfile.write(f\" - {line}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    add_prefix(\"bldg_sensors.txt\", \"bldg_sensors_sufix.txt\")\n",
    "    print(\"bldg_sensors_dash.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
